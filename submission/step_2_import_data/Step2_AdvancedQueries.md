# Step 2 Data Import advanced queries

### Query 1

**Description**: _Select author, title and content from all papers where the title contains the word "review"_

**Query string**: `$select=author,title,content&$filter=search.ismatch('review', 'title')`

**Index**: library

**Results**:

```json
{
  "@odata.context": "https://corporate-training-search.search.windows.net/indexes('papers-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 1,
      "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
      "author": "Alina Köchling "
    },
    {
      "@search.score": 1,
      "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often",
      "title": "Mining aspects of customer’s review on the social network",
      "author": "Tu Nguyen Thi Ngoc "
    },
    {
      "@search.score": 1,
      "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate ",
      "title": "Big data stream analysis: a systematic literature review",
      "author": "Taiwo Kolajo "
    },
    {
      "@search.score": 1,
      "content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research o",
      "title": "Technological advancements and opportunities in Neuromarketing: a systematic review",
      "author": "Ferdousi Sabera Rawnaque "
    }
  ]
}
```

---

### Query 2

**Description**: _Select author, title, content and keyphrases from all documents having the 'analysis' or 'ai' keyphrase_

**Query string**: `$select=author,title,content,keyphrases&$filter=keyphrases/any(kp: search.in(kp, 'analysis, ai'))`

**Index**: library

**Results**:

```json
{
  "@odata.context": "https://corporate-training-search.search.windows.net/indexes('papers-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 1,
      "content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A relatives",
      "keyphrases": [
        "computational trust models",
        "testbed",
        "experiments",
        "analysis"
      ],
      "title": "Toward a testbed for evaluating computational trust models: experiments and analysis",
      "author": "Partheeban Chandrasekaran"
    }
  ]
}
```

---

### Query 3

**Description**: _Select author, title, content and keyphrases from all documents where the content of the document matches 'artificial intelligence' and get the result count_

**Query string**: `$select=author,title,content,keyphrases&$filter=search.ismatch('artificial inteligence', 'content')&$count=true`

**Index**: library

**Results**:

```json
{
  "@odata.context": "https://corporate-training-search.search.windows.net/indexes('papers-index')/$metadata#docs(*)",
  "@odata.count": 8,
  "value": [
    {
      "@search.score": 1,
      "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "keyphrases": [
        "systematic review",
        "algorithmic decision-making",
        "HR recruitment",
        "HR development",
        "discrimination",
        "fairness",
        "context"
      ],
      "title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
      "author": "Alina Köchling "
    },
    {
      "@search.score": 1,
      "content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like precision i",
      "keyphrases": [],
      "title": null,
      "author": null
    },
    {
      "@search.score": 1,
      "content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nParlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence:   \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\n\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\n\nn\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C)−H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN (AD − CB)2\n\n(A+ C)(B+ D)(A+ B)(C + D)\n\n(5)χ2(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf\n+ is the number of documents in the positive class that contain feature f, DFf\n\n− \nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and mf, mc\n\nf are \nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc −mf\n\n)2\n\n(8)mc =\n\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1− qf\n)\n\n(\n\n1− pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere DFf\n+ and DFf\n\n− are the raw counts of documents that contain f in the positive and \nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs. 12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5 GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table 2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne \nb\n\nes\nt c\n\nla\nss\n\nifi\nca\n\nti\no\n\nn\n r\n\nes\nu\n\nlt\ns \n\nfo\nr \n\np\nai\n\nrs\n o\n\nf f\nea\n\ntu\nre\n\n s\nel\n\nec\nti\n\no\nn\n\n m\net\n\nh\no\n\nd\ns \n\nan\nd\n\n th\ne \n\nTu\nrk\n\nis\nh\n\n r\nev\n\nie\nw\n\n d\nat\n\nas\net\n\ns\n\nQ\nER\n\nD\nFD\n\nO\nC\n\nFS\nC\n\nH\nI2\n\nIG\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\n\nM\no",
      "keyphrases": [
        "new feature selection method",
        "sentiment analysis",
        "QER"
      ],
      "title": "QER: a new feature selection method for sentiment analysis",
      "author": "Tuba Parlar "
    },
    {
      "@search.score": 1,
      "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. In the",
      "keyphrases": [
        "Mobile marketing recommendation method",
        "user location feedback"
      ],
      "title": "Mobile marketing recommendation method based on user location feedback",
      "author": "Chunyong Yin "
    },
    {
      "@search.score": 1,
      "content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \n\nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nRESEARCH\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 × 32 = 512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ ti=0P(i) where the class probabilities of different gray level pixels were estimated \nas shown in Eqs. 2 and 3:\n\n(1)σ 2\nw(t) = q1(t)σ\n\n2\n1 (t)+ q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\n\nt\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\n\nt\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ 2\nb (t) = q1(t)[1− q1(t)][µ1(t)− µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F ][S]T\n\n(8)[F ] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\n\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k , v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj,u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12 ×  3 =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig. 3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 = 6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhi\nVF\n\nxhi += +\n..\n\n∑∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxlo\nVF\n\nxlo += +\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq. 14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\n\nn\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq. 15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an appropriate nonlinear",
      "keyphrases": [
        "content based image data",
        "Multi technique amalgamation",
        "information identification",
        "enhanced"
      ],
      "title": "Multi technique amalgamation for enhanced information identification with content based image data",
      "author": "Rik Das"
    },
    {
      "@search.score": 1,
      "content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′\n\ni is 0543216.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are",
      "keyphrases": [],
      "title": null,
      "author": null
    },
    {
      "@search.score": 1,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope wit",
      "keyphrases": [
        "Distributed Memory-based Resilient Dataset Filter",
        "Improving prediction"
      ],
      "title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter",
      "author": "Sandhya Narayanan "
    },
    {
      "@search.score": 1,
      "content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research o",
      "keyphrases": [
        "Technological advancements",
        "systematic review",
        "opportunities",
        "Neuromarketing"
      ],
      "title": "Technological advancements and opportunities in Neuromarketing: a systematic review",
      "author": "Ferdousi Sabera Rawnaque "
    }
  ]
}
```

---

### Query 4

**Description**: *Search for 'devops', select title, description, product, duration, rating_average, level, role and instructor, where rating_average is greater than 4.75 and duration is greater than 10, sort by rating_average descending and duration descending, also get result count*

**Query string**: `search=devops  AND level:'beginner' AND product eq 'azure'&queryType=full&$select=title,description,product,duration,rating_average,level,role,instructor&$filter=rating_average gt 3.5 and duration gt 10&$orderby rating_average desc, duration desc&$count=true`

**Index**: courses

**Results**:

```json
{
  "@odata.context": "https://corporate-training-search.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "@odata.count": 56,
  "value": [
    {
      "@search.score": 8.410371,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 8.410371,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 6.629801,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.79,
      "role": "developer",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 6.629801,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.79,
      "role": "developer",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 6.629801,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.79,
      "role": "solution-architect",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 6.629801,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.79,
      "role": "solution-architect",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 6.629801,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.79,
      "role": "administrator",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 6.629801,
      "description": "Learn to automate DevOps processes by using GitHub Apps that handle repetitive tasks, enforce team policies, and maintain a tidy repository.",
      "duration": 68,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.79,
      "role": "administrator",
      "title": "Automate DevOps processes by using GitHub Apps"
    },
    {
      "@search.score": 3.7787046,
      "description": "Implement a CI/CD pipeline for Python.",
      "duration": 46,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.78,
      "role": "devops-engineer",
      "title": "Automate Python deployments with Azure Pipelines"
    },
    {
      "@search.score": 3.7787046,
      "description": "Implement CI/CD pipeline for Node.js applications. Automate deployment of Vue, React, Angular or webpack apps with Azure Pipelines.",
      "duration": 44,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.9,
      "role": "devops-engineer",
      "title": "Automate Node.js deployments with Azure Pipelines"
    },
    {
      "@search.score": 3.7787046,
      "description": "Learn along with the Space Game web team the benefits of collaboration through Visual Studio Code and GitHub.",
      "duration": 87,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.77,
      "role": "devops-engineer",
      "title": "Implement a code workflow in your build pipeline by using Git and GitHub"
    },
    {
      "@search.score": 1.9981346,
      "description": "Implement CI/CD pipeline for Node.js applications. Automate deployment of Vue, React, Angular or webpack apps with Azure Pipelines.",
      "duration": 44,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.9,
      "role": "administrator",
      "title": "Automate Node.js deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.9981346,
      "description": "Implement a CI/CD pipeline for Python.",
      "duration": 46,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.78,
      "role": "developer",
      "title": "Automate Python deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.9981346,
      "description": "Learn along with the Space Game web team the benefits of collaboration through Visual Studio Code and GitHub.",
      "duration": 87,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.77,
      "role": "solution-architect",
      "title": "Implement a code workflow in your build pipeline by using Git and GitHub"
    },
    {
      "@search.score": 1.9981346,
      "description": "Implement a CI/CD pipeline for Python.",
      "duration": 46,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.78,
      "role": "solution-architect",
      "title": "Automate Python deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.9981346,
      "description": "Implement a CI/CD pipeline for Python.",
      "duration": 46,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.78,
      "role": "administrator",
      "title": "Automate Python deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.9981346,
      "description": "Learn along with the Space Game web team the benefits of collaboration through Visual Studio Code and GitHub.",
      "duration": 87,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.77,
      "role": "administrator",
      "title": "Implement a code workflow in your build pipeline by using Git and GitHub"
    },
    {
      "@search.score": 1.9981346,
      "description": "Implement CI/CD pipeline for Node.js applications. Automate deployment of Vue, React, Angular or webpack apps with Azure Pipelines.",
      "duration": 44,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.9,
      "role": "developer",
      "title": "Automate Node.js deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.9981346,
      "description": "Learn along with the Space Game web team the benefits of collaboration through Visual Studio Code and GitHub.",
      "duration": 87,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.77,
      "role": "developer",
      "title": "Implement a code workflow in your build pipeline by using Git and GitHub"
    },
    {
      "@search.score": 1.9981346,
      "description": "Implement CI/CD pipeline for Node.js applications. Automate deployment of Vue, React, Angular or webpack apps with Azure Pipelines.",
      "duration": 44,
      "instructor": "",
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.9,
      "role": "solution-architect",
      "title": "Automate Node.js deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to implement a release based workflow on GitHub using project boards, branches, and releases.",
      "duration": 104,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.78,
      "role": "devops-engineer",
      "title": "Manage software delivery by using a release based workflow on GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to move your existing project to GitHub from a legacy version control system.",
      "duration": 43,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.76,
      "role": "devops-engineer",
      "title": "Migrate your repository by using GitHub best practices"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "duration": 72,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.8,
      "role": "devops-engineer",
      "title": "Create and host web sites by using GitHub Pages"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement CI/CD pipeline for Node.js applications. Automate deployment of Vue, React, Angular or webpack apps with Azure Pipelines.",
      "duration": 44,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.9,
      "role": "devops-engineer",
      "title": "Automate Node.js deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to interact with the GitHub API from GitHub Actions by using GitHub Script.",
      "duration": 25,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.76,
      "role": "devops-engineer",
      "title": "Automate GitHub by using GitHub Script"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "duration": 54,
      "instructor": "",
      "level": "intermediate",
      "product": "github",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "duration": 54,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-container-registry",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn how to contribute to an open-source project on GitHub.",
      "duration": 27,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.88,
      "role": "devops-engineer",
      "title": "Contribute to an open-source project on GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to settle overlapping commits across branches by using merge conflict resolution.",
      "duration": 52,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.83,
      "role": "devops-engineer",
      "title": "Settle competing commits by using merge conflict resolution on GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "duration": 54,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-kubernetes-service",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": 38,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Search and organize repository history by using GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to settle overlapping commits across branches by using merge conflict resolution.",
      "duration": 52,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.83,
      "role": "devops-engineer",
      "title": "Settle competing commits by using merge conflict resolution on GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Azure Pipelines help automate building, deploying, and maintaining your applications. While they support a wide range of platforms and programming languages, in this module you’ll focus on using them to implement ASP.NET apps on Azure App Service Web Apps with Azure SQL Database as their data store.",
      "duration": 65,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.85,
      "role": "devops-engineer",
      "title": "Deploy ASP.NET web apps with Azure Pipelines"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to upload your existing project to GitHub.",
      "duration": 43,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.76,
      "role": "devops-engineer",
      "title": "Upload your project by using GitHub best practices"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement a CI/CD pipeline for Python.",
      "duration": 46,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.78,
      "role": "devops-engineer",
      "title": "Automate Python deployments with Azure Pipelines"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to move your existing project to GitHub from a legacy version control system.",
      "duration": 43,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.76,
      "role": "devops-engineer",
      "title": "Migrate your repository by using GitHub best practices"
    },
    {
      "@search.score": 1.78057,
      "description": "In this module, we'll authenticate to GitHub, create a local Git repository, and push the repository to GitHub by using the Git tooling experience in Visual Studio 2019. We'll add and modify files, stage and commit changes, and then finally push to your remote.",
      "duration": 38,
      "instructor": "",
      "level": "beginner",
      "product": "vs",
      "rating_average": 5,
      "role": "devops-engineer",
      "title": "Get started with Git and GitHub in Visual Studio"
    },
    {
      "@search.score": 1.78057,
      "description": "In this module, we'll authenticate to GitHub, create a local Git repository, and push the repository to GitHub by using the Git tooling experience in Visual Studio 2019. We'll add and modify files, stage and commit changes, and then finally push to your remote.",
      "duration": 38,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 5,
      "role": "devops-engineer",
      "title": "Get started with Git and GitHub in Visual Studio"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to interact with the GitHub API from GitHub Actions by using GitHub Script.",
      "duration": 25,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.76,
      "role": "devops-engineer",
      "title": "Automate GitHub by using GitHub Script"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to create a successful open-source program by establishing contributor guidance, following proven processes, and leveraging community standards.",
      "duration": 91,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.82,
      "role": "devops-engineer",
      "title": "Create an open-source program by using GitHub best practices"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to use the GitHub integration in Visual Studio Code, including authentication, publishing repos, and viewing your repo timeline.",
      "duration": 22,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.82,
      "role": "devops-engineer",
      "title": "Introduction to GitHub in Visual Studio Code"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn how to contribute to an open-source project on GitHub.",
      "duration": 27,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.88,
      "role": "devops-engineer",
      "title": "Contribute to an open-source project on GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "duration": 54,
      "instructor": "",
      "level": "intermediate",
      "product": "dotnet-core",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn how to contribute to an open-source project on GitHub.",
      "duration": 27,
      "instructor": "",
      "level": "beginner",
      "product": "vs",
      "rating_average": 4.88,
      "role": "devops-engineer",
      "title": "Contribute to an open-source project on GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "duration": 54,
      "instructor": "",
      "level": "intermediate",
      "product": "aspnet-core",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions"
    },
    {
      "@search.score": 1.78057,
      "description": "Overview GitHub's products, associated features, and licensing of per-use and metered features.",
      "duration": 19,
      "instructor": "",
      "level": "intermediate",
      "product": "github",
      "rating_average": 4.77,
      "role": "devops-engineer",
      "title": "Introduction to GitHub's Products"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to use Markdown to communicate with brevity, clarity, and expression.",
      "duration": 62,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.76,
      "role": "devops-engineer",
      "title": "Communicate effectively on GitHub by using Markdown"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": 38,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Search and organize repository history by using GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Learn to implement a release based workflow on GitHub using project boards, branches, and releases.",
      "duration": 104,
      "instructor": "",
      "level": "beginner",
      "product": "github",
      "rating_average": 4.78,
      "role": "devops-engineer",
      "title": "Manage software delivery by using a release based workflow on GitHub"
    },
    {
      "@search.score": 1.78057,
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "duration": 54,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.79,
      "role": "devops-engineer",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions"
    }
  ],
  "@odata.nextLink": "https://corporate-training-search.search.windows.net/indexes('courses-index')/docs?api-version=2021-04-30-Preview&search=devops&$select=title%2Cdescription%2Cproduct%2Cduration%2Crating_average%2Clevel%2Crole%2Cinstructor&$filter=rating_average%20gt%204.75%20and%20duration%20gt%2010&$orderby%20rating_average%20desc%2C%20duration%20desc=&$count=true&$skip=50"
}
```

---

### Query 5

**Description**: _Select courses containing 'azure' where title matches both 'artificial' and 'intelligence', sort by duration descending_

**Query string**: `search=azure&$filter=search.ismatch('artificial+intelligence', 'title')&$orderby duration desc`

**Index**: courses

**Results**:

```json
{
  "@odata.context": "https://corporate-training-search.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 2.9656026,
      "Key": "ms-learn7d70effe-470c-403c-98f4-dd0295dab8a8",
      "description": "This module provides an overview of Azure AI and demonstrates how Microsoft tools, services, and infrastructure can help make AI real for your organization, whether you want to unlock insights from your latent data with knowledge mining, develop your own AI models with machine learning, or build immersive apps using AI.",
      "duration": 59,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 4523,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Microsoft Azure Artificial Intelligence (AI) strategy and solutions",
      "url": "https://docs.microsoft.com/en-us/learn/modules/azure-artificial-intelligence/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Microsoft tools",
        "latent data",
        "knowledge mining",
        "machine learning",
        "immersive apps",
        "Azure AI",
        "AI models",
        "module",
        "overview",
        "services",
        "infrastructure",
        "organization",
        "insights"
      ]
    },
    {
      "@search.score": 2.9656026,
      "Key": "ms-learnda546e02-79d1-4ea6-ac52-f694037e97a6",
      "description": "This module provides an overview of Azure AI and demonstrates how Microsoft tools, services, and infrastructure can help make AI real for your organization, whether you want to unlock insights from your latent data with knowledge mining, develop your own AI models with machine learning, or build immersive apps using AI.",
      "duration": 59,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 4523,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Microsoft Azure Artificial Intelligence (AI) strategy and solutions",
      "url": "https://docs.microsoft.com/en-us/learn/modules/azure-artificial-intelligence/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Microsoft tools",
        "latent data",
        "knowledge mining",
        "machine learning",
        "immersive apps",
        "Azure AI",
        "AI models",
        "module",
        "overview",
        "services",
        "infrastructure",
        "organization",
        "insights"
      ]
    }
  ]
}
```

---

### Query 6

**Description**: *Select courses containing both 'language' and 'understanding' where level matches 'advanced' and duration is less than 20 and rating_average is greater than 3.5*

**Query string**: search=language+understanding&$filter=search.ismatch('advanced', 'level') and duration lt 20 and rating_average gt 3.5

**Index**: courses

**Results**:

```json
{
  "@odata.context": "https://corporate-training-search.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "@odata.count": 12,
  "value": [
    {
      "@search.score": 24.570278,
      "Key": "ms-learn1f252bc6-f722-4add-a681-09b888e4b71c",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-language-understanding",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "developer",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 24.570278,
      "Key": "ms-learnb5a079db-8f56-4414-8762-92ea7691d6bf",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-language-understanding",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 24.570278,
      "Key": "ms-learnba37fb77-137a-4538-a2fc-e41480b50f38",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-language-understanding",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 24.570278,
      "Key": "ms-learncdb546cc-adb3-4c45-8f73-2c98455e3379",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-language-understanding",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learn01731c10-20bb-41e7-ba21-8528669dcdc3",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learn871fd15d-0363-4b1c-a99d-2e9a14d3f652",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learn90f26b90-378d-49f3-b22d-f681e38e76e7",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learna72bbe5a-cb74-4cb7-a477-e8a3fc06c608",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-cognitive-services",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learnad5475aa-63b0-4b1c-ba9c-964cc3ad08fa",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-cognitive-services",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learnba89b5c5-fefd-459e-843e-0987a3dfbbd7",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-cognitive-services",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "developer",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learnc975e981-df18-44b6-9b16-e8ed3ed7de98",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure-cognitive-services",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    },
    {
      "@search.score": 19.413761,
      "Key": "ms-learndce8e311-b379-4016-9708-8a446004825c",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": 18,
      "instructor": "",
      "level": "advanced",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "developer",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Language Understanding Intelligent Service",
        "LUIS) Apps",
        "containers"
      ]
    }
  ]
}
```
